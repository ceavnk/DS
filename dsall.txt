#2 Data Frames and Basic Data Pre-processing:


import pandas as pd

# Read CSV
csv_file = 'PlayTENNIES.csv'
df_csv = pd.read_csv(csv_file)
print(df_csv.tail())
print("Shape:", df_csv.shape)
print("Columns:", df_csv.columns)
print("Dtypes:", df_csv.dtypes)

# Read JSON
json_file = 'sample_data.json'
df_json = pd.read_json(json_file)
print("JSON Data:\n", df_json.head())
print("Head(2):\n", df_json.head(2))
print("Tail(2):\n", df_json.tail(2))



print(df_csv)

# Method 1: Drop rows with missing values
df_cleaned = df_csv.dropna()
print("After dropping rows:\n", df_cleaned)

# Method 2: Drop columns with missing values
df_cleaned_col = df_csv.dropna(axis=1)
print("After dropping columns:\n", df_cleaned_col)

# Count missing values
print("Missing values:\n", df_csv.isnull().sum())

# Fill missing 'temp' with mode
df_csv['temp'].fillna(df_csv['temp'].mode()[0], inplace=True)
print("After filling 'temp' with mode:\n", df_csv)

# Fill missing 'sno' with mean
df_csv['sno'].fillna(df_csv['sno'].mean(), inplace=True)
print("After filling 'sno' with mean:\n", df_csv)



import matplotlib.pyplot as plt
import seaborn as sns

# Identify outliers
sns.boxplot(data=df_csv['no_of_people'])
plt.show()

# Remove extreme outliers
Q1 = df_csv['no_of_people'].quantile(0.25)
Q3 = df_csv['no_of_people'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

extreme_outliers = df_csv[(df_csv['no_of_people'] < lower_bound) | (df_csv['no_of_people'] > upper_bound)]
print("Extreme Outliers:\n", extreme_outliers)

df_cleaned = df_csv[(df_csv['no_of_people'] >= lower_bound) & (df_csv['no_of_people'] <= upper_bound)]
print("Cleaned Data:\n", df_cleaned)

# Boxplot after cleaning
sns.boxplot(data=df_cleaned['no_of_people'])
plt.show()





f_df = df_csv[df_csv['no_of_people'] > 2]
print("Filtered Data:\n", f_df)





s_df = df_csv.sort_values(by='no_of_people', ascending=False)
print("Sorted Data:\n", s_df)



g_df = df_csv.groupby('outlook').size()
print("Grouped Data:\n", g_df)











#3 Feature Scaling and Dummification: 

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

data = {
    'product': ['Apple_juice', 'Banana_Smoothie', 'Orange_Jam', 'Grape_Jelly', 'Kiwi_Parfait',
                'Mango_Chutney', 'Pineapple_Sorbet', 'Strawberry_yogurt', 'Blueberry_pie', 'Cherry_Sal'],
    'Category': ['Apple', 'Banana', 'Orange', 'Grape', 'Kiwi', 'Mango', 'Pipneapple', 'Strawberry', 'Blueberry', 'Cherry'],
    'Sales': [1200, 1700, 2200, 1400, 2000, 1000, 1500, 1800, 1300, 1600],
    'Cost': [600, 850, 1100, 700, 1000, 500, 750, 900, 650, 800],
    'Profit': [600, 850, 1100, 700, 1000, 500, 750, 900, 650, 800]
}

df = pd.DataFrame(data)

n_columns = ['Sales', 'Cost', 'Profit']
s_std = StandardScaler()
s_nor = MinMaxScaler()

df_s_std = pd.DataFrame(s_std.fit_transform(df[n_columns]), columns=n_columns)
df_s_nor = pd.DataFrame(s_nor.fit_transform(df[n_columns]), columns=n_columns)
df_scaled = pd.concat([df_s_std, df.drop(n_columns, axis=1)], axis=1)

print("Data after Feature Scaling:")
print(df_scaled)

categorical_columns = ['product', 'Category']

preprocessor = ColumnTransformer(
    transformers=[('categorical', OneHotEncoder(), categorical_columns)],
    remainder='passthrough'
)

df_dummified = pd.DataFrame(preprocessor.fit_transform(df).toarray())
df_dummified.columns = preprocessor.get_feature_names_out()

print("\nData after Feature Dummification:")
print(df_dummified)







#4 Hypothesis Testing




import scipy.stats as stats
import numpy as np

# One-sample t-test
g1 = [72,88,64,74,67,79,85,75,89,77]
assumed_mean = 70
t_stat, p_val = stats.ttest_1samp(g1, assumed_mean)
alpha = 0.05
df = len(g1) - 1
critical_t = stats.t.ppf(1 - alpha, df)
print("T-value:", t_stat, "P-value:", p_val, "Critical t-value:", critical_t)
if np.abs(t_stat) > critical_t:
    print("Reject H0: Mean is significantly different.")
else:
    print("Fail to reject H0: No significant difference.")

# Independent two-sample t-test
g1 = [85,95,100,80,90,97,104,95,88,92,94,99]
g2 = [83,85,96,92,100,104,94,95,88,90,93,94]
t_stat, p_val = stats.ttest_ind(g1, g2)
df = (len(g1) - 1) + (len(g2) - 1)
critical_t = stats.t.ppf(1 - alpha/2, df)
print("T-value:", t_stat, "P-value:", p_val, "Critical t-value:", critical_t)
if np.abs(t_stat) > critical_t:
    print("Reject H0: Significant difference between groups.")
else:
    print("Fail to reject H0: No significant difference between groups.")

# Paired t-test
g1 = [85,68,67,84,98,60,94,80,94,98,95,80,85,87,75]
g2 = [70,90,80,89,88,86,78,87,90,86,92,94,99,93,86]
t_stat, p_val = stats.ttest_rel(g1, g2)
df = len(g2) - 1
critical_t = stats.t.ppf(1 - alpha/2, df)
print("T-value:", t_stat, "P-value:", p_val, "Critical t-value:", critical_t)
if np.abs(t_stat) > critical_t:
    print("Reject H0: Significant difference between paired groups.")
else:
    print("Fail to reject H0: No significant difference between paired groups.")

# Chi-square test
from scipy.stats import chi2_contingency
contingency_table = [[50,75],[125,175],[90,30],[45,10]]
chi2_stat, p_val, dof, expected_table = chi2_contingency(contingency_table)
print("Chi-square statistic:", chi2_stat, "P-value:", p_val, "Degrees of freedom:", dof)
if p_val < alpha:
    print("Reject H0: Association exists between groups.")
else:
    print("Fail to reject H0: No association between groups.")








#5 ANOVA (Analysis of Variance)




dietData <- read.csv("c\download\90_Data_File.csv", header = TRUE, sep = ",")
print(dietData)

boxplot(dietData$Weight.Loss ~ dietData$Diet, data = dietData, col = "light blue", ylab = "Weight loss (kg)", xlab = "Diet type")

dietData$Diet <- as.factor(dietData$Diet)

table <- aov(dietData$Weight.Loss ~ dietData$Diet, data = dietData)
summary(table)

TukeyHSD(table)
plot(TukeyHSD(table))





#6Aim:-Regression and Its Types:-

import pandas as pd
import seaborn as sns
from sklearn import linear_model
import warnings
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Single Variable Regression
df = pd.read_csv(r'C:\Users\aksha\Downloads\house_pricing.csv')
warnings.filterwarnings('ignore')
sns.lmplot(x="Area", y="Price", data=df)
reg = linear_model.LinearRegression()
reg.fit(df[["Area"]], df[["Price"]])
reg.predict([[500]])
reg.coef_
reg.intercept_

# Multi-variable Regression
df = pd.read_csv(r"C:\Users\aksha\Downloads\House_Pricing_Multivariables.csv")
mean_f1 = round(df.Floors.mean())
df.Floors = df.Floors.fillna(mean_f1)
reg = linear_model.LinearRegression()
reg.fit(df[['Bedrooms', 'Bathrooms', 'Sqft_living', 'Floors', 'Grade', 'Sqft_above', 'Sqft_basement']], df['Price'])
reg.predict([[3, 2, 1180, 1, 7, 1180, 0]])

# Model Summary using Statsmodels
data = pd.read_csv(r"C:\Users\aksha\Downloads\homeprices.csv")
x = data[['area', 'bedrooms', 'age']]
y = data['price']
X = sm.add_constant(x)
model = sm.OLS(y, X).fit()
print(model.summary())

# Residual Plot
residuals = y - model.predict(X)
plt.scatter(model.predict(X), residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Price')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()







#7Logistic Regression and Decision Tree:

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier, export_text
import numpy as np

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([0, 0, 0, 1, 1])

model = LogisticRegression()
model.fit(X, y)

n_data = np.array([[4.5]])
y_pred = model.predict(n_data)
print("Prediction:", y_pred)

prob = model.predict_proba(n_data)
print("Predict Probabilities:", prob)

accuracy = accuracy_score(y, model.predict(X))
precision = precision_score(y, model.predict(X))
recall = recall_score(y, model.predict(X))
f1 = f1_score(y, model.predict(X))

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

conf_matrix = confusion_matrix(y, model.predict(X))
print("Confusion Matrix:\n", conf_matrix)

print("\nClassification Report:")
print(classification_report(y, model.predict(X)))

x = np.array([[1, 50], [2, 60], [3, 65], [4, 75], [5, 90]])
y = np.array([0, 0, 0, 1, 1])

model_tree = DecisionTreeClassifier()
model_tree.fit(x, y)

tree_rules = export_text(model_tree, feature_names=["Years_Experience", "Performance_Score"])
print("\nDecision Tree Rules:\n", tree_rules)

new_data = np.array([[4, 72]])
prediction = model_tree.predict(new_data)
print("\nPrediction for [4 Years, 72 Score]:", "Promoted" if prediction[0] == 1 else "Not Promoted")






#8Clustering with K-Means


from sklearn.cluster import KMeans
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
%matplotlib inline

df = pd.read_csv("income.csv")
plt.scatter(df.Age, df['Income($)'])
plt.xlabel('Age')
plt.ylabel('Income($)')




km = KMeans(n_clusters=3)
y_predicted = km.fit_predict(df[['Age', 'Income($)']])
df['cluster'] = y_predicted

df1 = df[df.cluster == 0]
df2 = df[df.cluster == 1]
df3 = df[df.cluster == 2]
plt.scatter(df1.Age, df1['Income($)'], color='green')
plt.scatter(df2.Age, df2['Income($)'], color='red')
plt.scatter(df3.Age, df3['Income($)'], color='black')
plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], color='purple', marker='*', label='centroid')
plt.xlabel('Age')
plt.ylabel('Income($)')
plt.legend()





scaler = MinMaxScaler()
df['Income($)'] = scaler.fit_transform(df[['Income($)']])
df['Age'] = scaler.fit_transform(df[['Age']])

plt.scatter(df.Age, df['Income($)'])




km = KMeans(n_clusters=3)
y_predicted = km.fit_predict(df[['Age', 'Income($)']])
df['cluster'] = y_predicted

df1 = df[df.cluster == 0]
df2 = df[df.cluster == 1]
df3 = df[df.cluster == 2]
plt.scatter(df1.Age, df1['Income($)'], color='green')
plt.scatter(df2.Age, df2['Income($)'], color='red')
plt.scatter(df3.Age, df3['Income($)'], color='black')
plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], color='purple', marker='*', label='centroid')
plt.legend()



sse = []
k_rng = range(1, 10)
for k in k_rng:
    km = KMeans(n_clusters=k)
    km.fit(df[['Age', 'Income($)']])
    sse.append(km.inertia_)

plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.plot(k_rng, sse)









#9Principal Component Analysis (PCA) Tutorial

from sklearn.datasets import load_digits
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
%matplotlib inline

dataset = load_digits()
dataset.keys()
dataset.data.shape
dataset.data[0]
dataset.data[0].reshape(8,8)

plt.gray()
plt.matshow(dataset.data[0].reshape(8,8))
plt.matshow(dataset.data[9].reshape(8,8))

dataset.target[:5]

df = pd.DataFrame(dataset.data, columns=dataset.feature_names)
df.head()
dataset.target
df.describe()

X = df
y = dataset.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)

model = LogisticRegression()
model.fit(X_train, y_train)
model.score(X_test, y_test)

pca = PCA(0.95)
X_pca = pca.fit_transform(X)
X_pca.shape
pca.explained_variance_ratio_
pca.n_components_

X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)

model = LogisticRegression(max_iter=1000)
model.fit(X_train_pca, y_train)
model.score(X_test_pca, y_test)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
X_pca.shape
X_pca
pca.explained_variance_ratio_

X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)

model = LogisticRegression(max_iter=1000)
model.fit(X_train_pca, y_train)
model.score(X_test_pca, y_test)
